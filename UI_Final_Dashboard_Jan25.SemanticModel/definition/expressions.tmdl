expression risk_df_python =
		let
		    Source = Python.Execute("import re#(lf)import ast#(lf)import json#(lf)import shap#(lf)import pandas as pd#(lf)import numpy as np#(lf)import seaborn as sns#(lf)from tqdm.notebook import tqdm#(lf)from lightgbm import LGBMClassifier#(lf)from xgboost import XGBClassifier#(lf)from sklearn.preprocessing import OrdinalEncoder#(lf)from sklearn.ensemble import RandomForestClassifier#(lf)from sklearn.model_selection import train_test_split#(lf)from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, confusion_matrix#(lf)from pandas._libs.tslibs.timestamps import Timestamp#(lf)import os#(lf)import warnings#(lf)warnings.filterwarnings(""ignore"")#(lf)pd.set_option(""future.no_silent_downcasting"", True)#(lf)import pyodbc#(lf)import pandas as pd#(lf)server = 'glenmark.database.windows.net'  # e.g., 'myserver.database.windows.net'#(lf)database = 'Glenmark'  # e.g., 'mydatabase'#(lf)username = 'Projects'  # your SQL server username#(lf)password = 'Dashboards@2024'  # your SQL server password#(lf)conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password}'#(lf)try:#(lf)    conn = pyodbc.connect(conn_str)#(lf)    print(""Connected to Azure SQL Server successfully!"")#(lf)    query = ""SELECT * FROM [stg_raw].[EmployeeData_HRMS]""  # Replace with your query#(lf)    df = pd.read_sql_query(query, conn)#(lf)    print(df)#(lf)    conn.close()#(lf)except Exception as e:#(lf)    print(""Error connecting to Azure SQL Server:"", e)#(lf)df = df.replace(r'^\s*--\s*$', np.nan, regex=True)#(lf)df = df.replace(r'^\s*-\s*$', np.nan, regex=True)#(lf)df.columns = df.columns.str.strip()#(lf)column_mapp = {'EmployeeID' : 'Employee ID', #(lf)               'EmploymentType' : 'Employment Type', #(lf)               'GradeCategory' :'Grade Category',#(lf)               'SubFunction' : 'Sub Function',#(lf)               'DateOfJoining' : 'Date of Joining',#(lf)               'DateOfExit' : 'Date of Exit',#(lf)               'DateOfBirth' : 'Date of Birth',#(lf)               'DateOfLastPromotion' : 'Date of Last Promotion',#(lf)               'ManagerID' : 'Manager ID',#(lf)               'ExitType' : 'Exit Type',#(lf)               'BU_FunctionLeader' : 'BU/Function Leader',#(lf)               'ClubbedFunctionTagging' : ""Clubbed Function Tagging"", #(lf)               'Year_2_Rating' : 'July 2022 rating',#(lf)               'PreviousYearRating' : 'July 2023 rating',#(lf)               'CurrentYearRating' : 'July 2024 rating',#(lf)               'TotalCompInINR' : 'Total Comp in INR',#(lf)               'LTI_CPLB_CurrentYear' : 'LTI/CPLB 2024',#(lf)               'LTI/CPLB2022' : 'LTI/CPLB 2022',#(lf)               'LTI/CPLB2023' : 'LTI/CPLB 2023',#(lf)               'ReportingLeader1' : 'Reporting Leader 1 (i.e. DR of Glenn)', #(lf)               'Total_Compensation' : 'Total Compensation',#(lf)               'Manager_employee_ID_(July 2023)' : 'Manager employee ID (July 2023)',#(lf)               'Manager_employee_ID_(July 2022)' : 'Manager employee ID (July 2022)',#(lf)               'July_2022_Fixed_Increase%' : 'July 2022 Fixed Increase %',#(lf)               'July_2023_Fixed_Increase%' : 'July 2023 Fixed Increase %',#(lf)               'July_2024_Fixed_Increase%' : 'July 2024 Fixed Increase %',#(lf)               'Fixed_Compensation' : 'Fixed Compensation',#(lf)              }#(lf)df.rename(columns = column_mapp, inplace = True)#(lf)columns = [#(lf)    'Employee ID', # keeping to maintain records later#(lf)    'Employment Type', # take Full Time only#(lf)    'Grade', # higher the better, CM > SM > SS#(lf)    'Grade Category', # higher the better#(lf)    'Country', # categorize#(lf)    'Location', # categroize#(lf)    'BU', # business unit, categroize#(lf)    'SBU', # sub-business unit, categroize#(lf)    'Function', # categroize#(lf)    'Sub Function', # categroize#(lf)    'KD', # categroize(Not present in Oct-2024 Data)#(lf)    'Date of Joining', # create tenure with DOE#(lf)    'Date of Exit', # replace with 1-Aug-2024 if null #(lf)    'Date of Birth', # create age#(lf)    'Date of Last Promotion', # day of last promotion; create tenure with DOE#(lf)    'Manager ID', # rename to 'Manager employee ID\n(July 2024) '#(lf)    'Manager employee ID (July 2023)', # create manager_changed: compare with T+1 manager, if (same or emp not in org )then false, else true#(lf)    'Manager employee ID (July 2022)',#(lf)    'Exit Type', # label col: only consider VOL; if VOL true, else false#(lf)    'BU/Function Leader', # categorize#(lf)    'Clubbed Function Tagging', # categorize#(lf)    'July 2022 Fixed Increase %', # value if emp present in the year, else 0#(lf)    'July 2023 Fixed Increase %',#(lf)    'July 2024 Fixed Increase %',#(lf)    'Fixed Compensation',#(lf)    'Total Compensation',#(lf)    'Total Comp in INR',#(lf)    'LTI/CPLB 2022', # long term incentive; value if emp present in the year, else 0#(lf)    'LTI/CPLB 2023',#(lf)    'LTI/CPLB 2024',]#(lf)#(lf)df = df[columns]#(lf)match_value_dict = {#(lf)    'Employment Type': 'Full Time'#(lf)}#(lf)skip_value_dict = {#(lf)    'BU/Function Leader': 'CEO (Old - Don\'t Report)',#(lf)    'Exit Type': 'INVOL'#(lf)}#(lf)non_null_columns = [#(lf)    'BU/Function Leader',#(lf)    'Clubbed Function Tagging' # there was only one row, can be imputed later instead#(lf)]#(lf)match_value_dict.items()#(lf)for key, val in match_value_dict.items():#(lf)    df = df[df[key] == val].drop(key, axis = 1) # should not need the column if using only one value from it#(lf)for key, val in skip_value_dict.items():#(lf)    df = df[df[key] != val]#(lf)for col in non_null_columns:#(lf)    df = df[df[col].isna() == False]#(lf)df = df.reset_index(drop = True)#(lf)#(lf)from datetime import datetime#(lf)current_date = datetime.now().date().strftime(""%d-%m-%Y"") #'01-08-2024'#(lf)current_date_ts = pd.to_datetime(current_date, errors='coerce',format='%d-%m-%Y', dayfirst=True)#(lf)#(lf)datetime_cols = [#(lf)    'Date of Joining',#(lf)    'Date of Exit',#(lf)    'Date of Birth',#(lf)    'Date of Last Promotion'#(lf)]#(lf)for col in datetime_cols:#(lf)    df[col] = pd.to_datetime(df[col], errors='coerce')#(lf)#(lf)#first_year = datetime.now().year - 2 #(lf)first_year = (datetime.now().year #(lf)    if df['Date of Joining'].max().year >= datetime.now().year #(lf)    else df['Date of Joining'].max().year) - 2#(lf)#(lf)last_year = (datetime.now().year #(lf)    if df['Date of Joining'].max().year >= datetime.now().year #(lf)    else df['Date of Joining'].max().year)#(lf)#(lf)presence_years = range(first_year, last_year + 1)#(lf)past_years = range(first_year, last_year)#(lf)#(lf)print(df[""Date of Exit""].unique())#(lf)print(df[""Date of Last Promotion""].unique())#(lf)df['Date of Exit'] = df['Date of Exit'].fillna(current_date_ts)#(lf)df['Date of Last Promotion'] = df['Date of Last Promotion'].fillna(df['Date of Joining'])#(lf)print(df[""Date of Exit""].unique())#(lf)print(df[""Date of Last Promotion""].unique())#(lf)comparision_date = pd.to_datetime(f'31-12-{first_year-1}',  errors='coerce', dayfirst = True)#(lf)print(comparision_date)#(lf)df = df[df['Date of Exit'] > comparision_date].reset_index(drop = True)#(lf)print(""Now the length of our Data from 2022 is "",len(df))#(lf)df['Tenure'] = pd.TimedeltaIndex(df['Date of Exit'] - df['Date of Joining']).days.astype(np.int16)#(lf)df['Days Since Promotion'] = pd.TimedeltaIndex(df['Date of Exit'] - df['Date of Last Promotion']).days.astype(np.int16)#(lf)df['Tenure'].unique()#(lf)#(lf)benefits_column_format = 'Applicable For Benefits {0}'#(lf)benefits_columns = [benefits_column_format.format(year) for year in presence_years]#(lf)benefits_columns#(lf)df[benefits_column_format.format('Overall')] = False#(lf)for year in presence_years:#(lf)    cutoff_date = pd.to_datetime(f'01/07/{year}', format='mixed')#(lf)    df[benefits_column_format.format(year)] = (df['Date of Exit'] > cutoff_date) & (pd.TimedeltaIndex(cutoff_date - df['Date of Joining']).days >= 365)#(lf)    df[benefits_column_format.format('Overall')] = df[benefits_column_format.format('Overall')] | df[benefits_column_format.format(year)]#(lf)#(lf)fixed_increase_column_format = 'July {} Fixed Increase %'#(lf)fixed_increase_columns = [fixed_increase_column_format.format(year) for year in presence_years]#(lf)fixed_increase_columns#(lf)for year in presence_years:#(lf)    col = fixed_increase_column_format.format(year)#(lf)    benefit_applicable = df[benefits_column_format.format(year)] == True#(lf)    df.loc[benefit_applicable, col] = df.loc[benefit_applicable, col].fillna('0%')#(lf)df[fixed_increase_columns] = df[fixed_increase_columns].replace('%', '', regex=True).astype(float)#(lf)df['Average Fixed Increase %'] = df[fixed_increase_columns].mean(axis = 1)#(lf)df['Average Fixed Increase %'] = df['Average Fixed Increase %'].fillna(0)#(lf)"""""" skip for now """"""#(lf)pass#(lf)lti_column_format = 'LTI/CPLB {}'#(lf)lti_columns = [lti_column_format.format(year) for year in presence_years]#(lf)lti_mapping = {r'^\s*YES\s*$': True, r'^\s*NO\s*$': False}#(lf)for year in presence_years:#(lf)    col = lti_column_format.format(year)#(lf)    benefit_applicable = df[benefits_column_format.format(year)] == True#(lf)    df.loc[benefit_applicable, col] = df.loc[benefit_applicable, col].fillna('NO')#(lf)df[lti_columns] = df[lti_columns].replace(lti_mapping, regex = True).astype(float)#(lf)df['Average LTI'] = df[lti_columns].mean(axis = 1)#(lf)df['Average LTI'] = df['Average LTI'].fillna(0)#(lf)df = df.drop(datetime_cols, axis = 1)#(lf)df = df.drop(benefits_columns, axis = 1)#(lf)df = df.drop(fixed_increase_columns, axis = 1)#(lf)df = df.drop(lti_columns, axis = 1)#(lf)df = df.rename(columns={'Manager ID': 'Manager employee ID (July 2024)'})#(lf)manager_column_format = 'Manager employee ID (July {0})'#(lf)manager_cols = [manager_column_format.format(year) for year in presence_years]#(lf)df = df.drop(manager_cols, axis = 1)#(lf)df = df[df['Total Comp in INR'].isna() == False].reset_index(drop = True)#(lf)numerical_columns = ['Fixed Compensation', 'Total Compensation', 'Total Comp in INR']#(lf)for col in numerical_columns:#(lf)    df[col] = df[col].astype(float).astype(np.int64)  # Round to the nearest integer and convert#(lf)df['Fixed Salary % of Total Comp'] = df['Fixed Compensation'] / df['Total Compensation'] * 100#(lf)Tenure_Buckets = [0, 1000, 2000, 4000, 6000, 8000, 16000]  # Now as integers#(lf)bins = [-float('inf')] + Tenure_Buckets + [float('inf')]#(lf)df['Tenure Buckets'] = pd.cut(df['Tenure'], bins=bins, labels=False)#(lf)Team_Columns = ['Grade', 'Country', 'Function']#(lf)team_col_data = [df[val] for val in Team_Columns]#(lf)df['Team'] = df.groupby(by=team_col_data).ngroup()#(lf)df['Average Team Compensation'] = df['Total Comp in INR'].groupby(by=df['Team']).transform('mean')#(lf)df['Difference from Average Team Compensation'] = df['Total Comp in INR'] - df['Average Team Compensation']#(lf)df['Average Team Increment'] = df['Average Fixed Increase %'].groupby(by=df['Team']).transform('mean')#(lf)df['Difference from Average Team Increment'] = df['Average Fixed Increase %'] - df['Average Team Increment']#(lf)df#(lf)df = df.drop(['Fixed Compensation', 'Total Compensation'], axis = 1)#(lf)df = df.drop('Team', axis = 1)#(lf)target_col = 'Exit Type'#(lf)df['Separated'] = df[target_col].fillna(False).replace('VOL', True).astype(np.int8)#(lf)df = df.drop('Exit Type', axis = 1)#(lf)ordinal_columns = [#(lf)    'Grade',#(lf)    'Grade Category'#(lf)]#(lf)categories = [list(df[col].unique()) for col in ordinal_columns]#(lf)encoder = OrdinalEncoder(categories=categories)#(lf)df[ordinal_columns] = encoder.fit_transform(df[ordinal_columns])#(lf)nominal_columns = [#(lf)    'Country',#(lf)    'Location',#(lf)    'BU',#(lf)    'SBU',#(lf)    'Function',#(lf)    'Sub Function',#(lf)    'KD',#(lf)    'BU/Function Leader',#(lf)    'Clubbed Function Tagging',]#(lf)def findCorrelation(corr, cutoff=0.9, exact=None):#(lf)    def _findCorrelation_fast(corr, avg, cutoff):#(lf)        combsAboveCutoff = corr.where(lambda x: (np.tril(x)==0) & (x > cutoff)).stack().index#(lf)        rowsToCheck = combsAboveCutoff.get_level_values(0)#(lf)        colsToCheck = combsAboveCutoff.get_level_values(1)#(lf)        msk = avg[colsToCheck] > avg[rowsToCheck].values#(lf)        deletecol = pd.unique(np.r_[colsToCheck[msk], rowsToCheck[~msk]]).tolist()#(lf)        return deletecol#(lf)    def _findCorrelation_exact(corr, avg, cutoff):#(lf)        x = corr.loc[(*[avg.sort_values(ascending=False).index]*2,)]#(lf)        if (x.dtypes.values[:, None] == ['int64', 'int32', 'int16', 'int8']).any():#(lf)            x = x.astype(float)#(lf)        x.values[(*[np.arange(len(x))]*2,)] = np.nan#(lf)        deletecol = []#(lf)        for ix, i in enumerate(x.columns[:-1]):#(lf)            for j in x.columns[ix+1:]:#(lf)                if x.loc[i, j] > cutoff:#(lf)                    if x[i].mean() > x[j].mean():#(lf)                        deletecol.append(i)#(lf)                        x.loc[i] = x[i] = np.nan#(lf)                    else:#(lf)                        deletecol.append(j)#(lf)                        x.loc[j] = x[j] = np.nan#(lf)        return deletecol#(lf)    if not np.allclose(corr, corr.T) or any(corr.columns!=corr.index):#(lf)        raise ValueError(""correlation matrix is not symmetric."")#(lf)    acorr = corr.abs()#(lf)    avg = acorr.mean()#(lf)    if exact or exact is None and corr.shape[1]<100:#(lf)        return _findCorrelation_exact(acorr, avg, cutoff)#(lf)    else:#(lf)        return _findCorrelation_fast(acorr, avg, cutoff)#(lf)non_nominal_cols = list(set(df.columns) - set(nominal_columns))#(lf)non_nominal_cols.remove('Separated')#(lf)cutoff = 0.95#(lf)corr = df[non_nominal_cols].corr()#(lf)highly_corr_cols = findCorrelation(corr)#(lf)df = df.drop(highly_corr_cols, axis = 1)#(lf)df[nominal_columns] = df[nominal_columns].replace('[^A-Za-z0-9_]+', '', regex = True)#(lf)one_hot_encodings = [pd.get_dummies(df[col], prefix=col) for col in nominal_columns]#(lf)df = pd.concat([df] + one_hot_encodings, axis = 1)#(lf)df = df.drop(nominal_columns, axis = 1)#(lf)emp_ids = df['Employee ID']#(lf)df = df.drop('Employee ID', axis = 1)#(lf)df['Separated'].value_counts()#(lf)def get_metrics(labels, preds, verbose = True):#(lf)    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()#(lf)    accuracy = accuracy_score(labels, preds)#(lf)    recall = recall_score(labels, preds)#(lf)    precision = precision_score(labels, preds)#(lf)    f1 = f1_score(labels, preds)#(lf)    roc = roc_auc_score(labels, preds)#(lf)    if verbose:#(lf)        print(f'TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}')#(lf)        print(f'{""Accuracy"":<20}: {accuracy}')#(lf)        print(f'{""Recall"":<20}: {recall}')#(lf)        print(f'{""Precision"":<20}: {precision}')#(lf)        print(f'{""F1-Score"":<20}: {f1}')#(lf)        print(f'{""ROC AUC"":<20}: {roc}')#(lf)    return {#(lf)        'accuracy': accuracy,#(lf)        'recall': recall,#(lf)        'precision': precision, #(lf)        'f1': f1, #(lf)        'roc': roc#(lf)    }#(lf)X, y = df.drop('Separated', axis = 1), df['Separated']#(lf)class ModelWrapper:#(lf)    def __init__(self, model):#(lf)        """"""#(lf)        model: Instance of initialized forest based model#(lf)        """"""#(lf)        self.model = model#(lf)        self.__rename_columns = type(self.model) is LGBMClassifier#(lf)    def __preprocess(self, df):#(lf)        return df.rename(columns = lambda col:re.sub('[^A-Za-z0-9_]+', '', col))#(lf)    def fit(self, X, y):#(lf)        if self.__rename_columns:#(lf)            X = self.__preprocess(X)#(lf)        self.model.fit(X, y)#(lf)    def predict(self, X):#(lf)        if self.__rename_columns:#(lf)            X = self.__preprocess(X)#(lf)        return self.model.predict(X)#(lf)    def predict_proba(self, X):#(lf)        if self.__rename_columns:#(lf)            X = self.__preprocess(X)#(lf)        return self.model.predict_proba(X)#(lf)    def get_important_features(self, num_features = 10, show_graph = False):#(lf)        important_features = sorted(zip(self.model.feature_importances_, #(lf)                                        self.model.feature_names_in_), reverse=True)[:num_features]#(lf)        sorted_importances, sorted_features = zip(*important_features)#(lf)        if show_graph: sns.barplot(x = sorted_importances, y = sorted_features)#(lf)        return pd.DataFrame({#(lf)            'Feature': sorted_features,#(lf)            'Relative Importance': sorted_importances#(lf)        })#(lf)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)#(lf)def get_best_model(X, y, metric = 'f1'):#(lf)    model_dict = {#(lf)        'Random Forest': RandomForestClassifier(n_estimators=500, random_state=42),#(lf)        'LGBM Classifier': LGBMClassifier(n_estimators=2000, is_unbalance=True, random_state=42),#(lf)        'XGBoost Classifier': XGBClassifier(n_estimators = 2000, random_state=42)#(lf)    }#(lf)    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)#(lf)    metrics = {}#(lf)    for model_name, model in model_dict.items():#(lf)        clf = ModelWrapper(model)#(lf)        clf.fit(X_train, y_train)#(lf)        preds = clf.predict(X_test)#(lf)        results = get_metrics(y_test, preds, verbose = False)#(lf)        metrics[model] = {#(lf)            'results': results,#(lf)            'score': results[metric],#(lf)            'model': clf#(lf)        }#(lf)    best_model = max(metrics, key=lambda model: metrics[model]['score'])#(lf)    print(f'Best model: {best_model}')#(lf)    return metrics[best_model]#(lf)best_model_details = get_best_model(X, y)#(lf)best_model_details#(lf)best_model = best_model_details['model']#(lf)best_df = best_model.get_important_features()#(lf)def add_spaces(string):#(lf)    # Add space before every uppercase letter that follows a lowercase letter#(lf)    string = re.sub(r'([a-z])([A-Z])', r'\1 \2', string)#(lf)    # Add space before 'from' and 'of' only when they are combined with the previous word#(lf)    string = re.sub(r'([a-zA-Z])(from|of)([A-Z])', r'\1 \2 \3', string)#(lf)    string = re.sub(r'([a-zA-Z])(from|of)\b', r'\1 \2', string)  # Handling cases like ""FixedSalaryofTotalComp""#(lf)    return string#(lf)best_df['Feature'] = best_df['Feature'].apply(add_spaces)#(lf)best_df#(lf)best_model.get_important_features(show_graph = True)#(lf)idx = df[df['Separated'] == 0].drop('Separated', axis = 1).index#(lf)X_present = df.loc[idx].drop('Separated', axis = 1).reset_index(drop = True)#(lf)present_emp_ids = list(emp_ids[idx])#(lf)probs = best_model.predict_proba(X_present)#(lf)probs#(lf)risk_df = pd.DataFrame({#(lf)    'Employee ID': present_emp_ids,#(lf)    'Risk %': probs[:, 1] * 100#(lf)})#(lf)risk_df['Risk Category'] = pd.cut(risk_df['Risk %'], #(lf)                 bins=[0, 10, 24, 100], # define bins for risk here#(lf)                 labels=['Low', 'Medium', 'High'])#(lf)#(lf)bool_cols = X.select_dtypes(include='bool').columns#(lf)X_explain = X.copy()#(lf)X_explain[bool_cols] = X_explain[bool_cols].astype(int)#(lf)explainer = shap.TreeExplainer(best_model.model, X_explain)#(lf)high_risk_idx = risk_df[risk_df['Risk Category'] == 'High'].index#(lf)shap_values = explainer.shap_values(X_present.loc[high_risk_idx],check_additivity=False) #ExplainerError: Additivity check failed in TreeExplainer! added check_additivity=False#(lf)for i, col in enumerate(X.columns):#(lf)    if col in non_nominal_cols:#(lf)        risk_df[f'{col} Factor'] = 'N/A'#(lf)        risk_df.loc[high_risk_idx, f'{col} Factor'] = shap_values[:, i]#(lf)for col in nominal_columns:#(lf)    col_idxs = np.where(X.columns.str.startswith('Country'))[0]#(lf)    if len(col_idxs) == 0: continue#(lf)    risk_df[f'{col} Factor'] = 'N/A'#(lf)    risk_df.loc[high_risk_idx, f'{col} Factor'] = shap_values[:, col_idxs].sum(axis = -1)#(lf)def get_top_factors(factors, k = 5):#(lf)    top_factors = sorted(factors, key=lambda col: abs(factors[col]), reverse = True)[:k]#(lf)    return str({k: factors[k] for k in top_factors})#(lf)risk_df['Factor Summary'] = 'N/A'#(lf)for idx in high_risk_idx: # not scalable at all#(lf)    risk_df.loc[idx, 'Factor Summary'] = get_top_factors(dict(risk_df.loc[idx][3:-1]))#(lf)def get_highest_factor(factor_summary):#(lf)    if pd.isna(factor_summary):#(lf)        return np.nan#(lf)    try:#(lf)        factor_dict = ast.literal_eval(factor_summary)#(lf)        max_key = max(factor_dict, key=factor_dict.get)#(lf)        return max_key#(lf)    except (ValueError, SyntaxError):#(lf)        return np.nan#(lf)risk_df['Likely Reason'] = risk_df['Factor Summary'].apply(get_highest_factor)#(lf)risk_df = risk_df[[""Employee ID"", ""Risk %"", ""Risk Category"", ""Likely Reason""]]#(lf)risk_df"),
		    risk_df1 = Source{[Name="risk_df"]}[Value],
		    #"Changed Type" = Table.TransformColumnTypes(risk_df1,{{"Employee ID", Int64.Type}, {"Risk %", type number}, {"Risk Category", type text}, {"Likely Reason", type text}})
		in
		    #"Changed Type"
	lineageTag: 4f38f5c8-54c6-4fad-bd1c-4d6962a0f569

	annotation PBI_NavigationStepName = Navigation

	annotation PBI_ResultType = Table

